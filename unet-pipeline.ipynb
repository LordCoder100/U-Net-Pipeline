{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import cv2 \n",
    "import shutil\n",
    "import zipfile \n",
    "import copy\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "from numpy import linalg as LA \n",
    "from math import atan2, cos, sin, sqrt, pi, log\n",
    "from PIL import Image \n",
    "from tqdm import tqdm  \n",
    "from torchvision import transforms \n",
    "from typing import List, Tuple\n",
    "from os import PathLike\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing functions for dataset images and masks. Original size is 4054x3040 pixels \n",
    "\n",
    "I will resize it to the following sizes: 128x128, 256x256, 512x512, 1024x1024 and 2048x2048 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(zip_archive: PathLike) -> None:\n",
    "    output_dir: str = \"unzipped-dataset\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_archive, \"r\") as zp:\n",
    "        zp.extractall(output_dir)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def move_files(source: PathLike, target_images: PathLike, target_masks: PathLike) -> None:\n",
    "    for file in os.listdir(source):\n",
    "        file_path = os.path.join(source, file)\n",
    "        if file.endswith(\".jpg\"):\n",
    "            shutil.move(file_path, os.path.join(target_images, file))\n",
    "        elif file.endswith(\".png\"):\n",
    "            shutil.move(file_path, os.path.join(target_masks, file))\n",
    "        else:\n",
    "            print(\"Unexpected file format found: Neither JPG nor PNG\")\n",
    "    \n",
    "    return None \n",
    "\n",
    "\n",
    "def organize_data(unzipped_data: PathLike) -> None:\n",
    "    train_dir: str = os.path.join(unzipped_data, \"train\")\n",
    "    val_dir: str = os.path.join(unzipped_data, \"val\")\n",
    "\n",
    "    target_train_images_dir: str = os.path.join(train_dir, \"images\")\n",
    "    target_train_masks_dir: str = os.path.join(train_dir, \"masks\")\n",
    "\n",
    "    target_val_images_dir: str = os.path.join(val_dir, \"images\")\n",
    "    target_val_masks_dir: str = os.path.join(val_dir, \"masks\")\n",
    "\n",
    "    os.makedirs(target_train_images_dir, exist_ok=True)\n",
    "    os.makedirs(target_train_masks_dir, exist_ok=True)\n",
    "    os.makedirs(target_val_images_dir, exist_ok=True)\n",
    "    os.makedirs(target_val_masks_dir, exist_ok=True)\n",
    "\n",
    "    move_files(train_dir, target_train_images_dir, target_train_masks_dir)\n",
    "    move_files(val_dir, target_val_images_dir, target_val_masks_dir)\n",
    "\n",
    "    print(\"Files have been successfully moved\")\n",
    "\n",
    "    return None \n",
    "\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir: PathLike, masks_dir: PathLike, size: int):\n",
    "        super(SegmentationDataset, self).__init__()\n",
    "        self.images_paths = [os.path.join(images_dir, file) for file in os.listdir(images_dir)]\n",
    "        self.masks_paths = [os.path.join(masks_dir, file) for file in os.listdir(masks_dir)]\n",
    "        self.size = size  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and correspoding mask paths \n",
    "        image_path = self.images_paths[idx]\n",
    "        mask_path = self.masks_paths[idx]\n",
    "\n",
    "        # Read image and mask file from the given path \n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "\n",
    "        # Convert BGR to RGB \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resize image and mask with the given size as tuple \n",
    "        image = cv2.resize(image, (self.size, self.size), cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, (self.size, self.size), cv2.INTER_NEAREST)\n",
    "\n",
    "        # Convert to PyTorch Tensor object to make it eligible for passing to the model \n",
    "        image = torch.from_numpy(image).unsqueeze(0).float() / 255.0\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define U-Net Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConvolution(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "\n",
    "        return down, p\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConvolution(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConvolution(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down_1, p1 = self.down_convolution_1(x)\n",
    "        down_2, p2 = self.down_convolution_2(p1)\n",
    "        down_3, p3 = self.down_convolution_3(p2)\n",
    "        down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        up_1 = self.up_convolution_1(b, down_4)\n",
    "        up_2 = self.up_convolution_2(up_1, down_3)\n",
    "        up_3 = self.up_convolution_3(up_2, down_2)\n",
    "        up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: DICE and Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(prediction, target, epsilon=1e-07) -> float:\n",
    "    prediction_copy = prediction.clone()\n",
    "    prediction_copy[prediction_copy < 0] = 0 \n",
    "    prediction_copy[prediction_copy > 0] = 1\n",
    "\n",
    "    intersection = abs(torch.sum(prediction_copy * target))\n",
    "    union = abs(torch.sum(prediction_copy) + torch.sum(target))\n",
    "\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "\n",
    "    return dice.item()  \n",
    "\n",
    "def iou_coefficient(prediction, target, epsilon=1e-07) -> float:\n",
    "    prediction = (prediction > epsilon).float()\n",
    "\n",
    "    intersection = (prediction * target).sum(dim=(1, 2))\n",
    "    union = (prediction + target).clamp(0, 1).sum(dim=(1, 2))\n",
    "\n",
    "    iou_score = intersection / union \n",
    "\n",
    "    return iou_score.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, optimizer: optim.Optimizer, trainingdataloader: DataLoader, validationdataloader: DataLoader, epochs: int, experiment_name: str, device: torch.device, tag: str) -> None:\n",
    "    # Paths to checkpoints, logging directory to track model perfomance, criterion(loss function) and other metrics/components/paths including Tensorboard writer object \n",
    "    logging_directory: str = f\"unet-runs/{experiment_name}\"\n",
    "    checkpoints: str = os.path.join(logging_directory, experiment_name, \"ckpt\")\n",
    "    save_path_best: str = os.path.join(checkpoints, f\"best_{tag}.pt\")\n",
    "    save_path_last: str = os.path.join(checkpoints, f\"last_{tag}.pt\")\n",
    "    tb_writer: SummaryWriter = SummaryWriter(log_dir=logging_directory)\n",
    "    criterion: nn.Module = nn.BCEWithLogitsLoss()\n",
    "    best_loss: float = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Model training \n",
    "        model.train()\n",
    "        train_running_loss: float = 0.0 \n",
    "        train_running_dice: float = 0.0 \n",
    "        train_running_iou: float = 0.0\n",
    "\n",
    "        for idx, img_mask in enumerate(tqdm(trainingdataloader, position=0, leave=True)):\n",
    "            img = img_mask[0].float().to(device)\n",
    "            mask = img_mask[1].float().to(device)\n",
    "\n",
    "            segmentations = model(img)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dc = dice_coefficient(segmentations, mask)\n",
    "            iou = iou_coefficient(segmentations, mask)\n",
    "            loss = criterion(segmentations, mask)\n",
    "\n",
    "            train_running_loss += loss.item()\n",
    "            train_running_dice += dc.item()\n",
    "            train_running_iou += iou.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss = train_running_loss / (idx + 1)\n",
    "        train_dice = train_running_dice / (idx + 1)\n",
    "        train_iou = train_running_iou / (idx + 1)\n",
    "\n",
    "        # Add metrics to Tensorboard like IOU score, DICE and training loss values \n",
    "        tb_writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"DICE/Train\", train_dice, epoch + 1)\n",
    "        tb_writer.add_scalar(\"IOU/Train\", train_iou, epoch + 1)\n",
    "\n",
    "        # Add training segmentation results as images  \n",
    "        tb_writer.add_images(\"Original mask\", img_mask, epoch + 1)\n",
    "        tb_writer.add_images(\"Segmentation mask\", segmentations, epoch + 1)\n",
    "\n",
    "\n",
    "        # Model validation \n",
    "        model.eval()\n",
    "        val_running_loss: float = 0.0 \n",
    "        val_running_dice: float = 0.0 \n",
    "        val_running_iou: float = 0.0 \n",
    "\n",
    "        for idx, val_img_mask in enumerate(tqdm(validationdataloader, position=0, leave=True)):\n",
    "            val_img = val_img_mask[0].float().to(device)\n",
    "            val_mask = val_img_mask[1].float().to(device)\n",
    "            val_segmentations = model(img)\n",
    "\n",
    "            dc_val = dice_coefficient(val_segmentations, val_mask)\n",
    "            iou_val = iou_coefficient(val_segmentations, val_mask)\n",
    "            val_loss = criterion(val_segmentations, val_mask)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "            val_running_dice += dc_val.item()\n",
    "            val_running_iou += iou_val.item()\n",
    "        \n",
    "        validation_loss = val_running_loss / (idx + 1)\n",
    "        validation_dice = val_running_dice / (idx + 1)\n",
    "        validation_iou = val_running_iou / (idx + 1)\n",
    "\n",
    "        # Add validation metrics like IOU, Loss and DICE \n",
    "        tb_writer.add_scalar(\"Loss/Validation\", validation_loss, epoch + 1)\n",
    "        tb_writer.add_scalar(\"DICE/Validation\", validation_dice, epoch + 1)\n",
    "        tb_writer.add_scalar(\"IOU/Validation\", validation_iou, epoch + 1)\n",
    "\n",
    "        # Add validation segmentation results as images\n",
    "        tb_writer.add_images(\"Validation original mask\", val_img_mask, epoch + 1)\n",
    "        tb_writer.add_images(\"Validation segmentation mask\", val_segmentations, epoch + 1)\n",
    "\n",
    "        # Save best.pt and last.pt checkpoints \n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            torch.save(model.state_dict(), save_path_best)\n",
    "        \n",
    "        torch.save(model.state_dict(), save_path_last)\n",
    "        \n",
    "\n",
    "    tb_writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path: str = \"\" # path to GlobalLogic's Google Drive \n",
    "\n",
    "unzip_data(data_path) # Unzip zip-archive to get train/val folders called dataset-splitted/train for train_images and masks and dataset-splitted/val for validation images and masks \n",
    "\n",
    "unzipped_path: str = \"\"  # path to unzipped folder called dataset-splitted \n",
    "\n",
    "organize_data(unzipped_path) # Sort images and masks into subfolder images and masks inside dataset-splitted folder \n",
    "\n",
    "size_128 = 128*128 \n",
    "size_256 = 256*256 \n",
    "size_512 = 512*512 \n",
    "size_1024 = 1024*1024\n",
    "\n",
    "path_to_train_images = \"\" # path to training images \n",
    "path_to_train_masks = \"\" # path to training masks \n",
    "path_to_val_images = \"\" # path to validation images \n",
    "path_to_val_masks = \"\" # path to validation masks \n",
    "\n",
    "num_workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 300\n",
    "\n",
    "# Datasets prepared for work \n",
    "\n",
    "train_dataset_128 = SegmentationDataset(path_to_train_images, path_to_train_masks, size_128)\n",
    "val_dataset_128 =  SegmentationDataset(path_to_val_images, path_to_train_masks, size_128)\n",
    "\n",
    "train_dataset_256 = SegmentationDataset(path_to_train_images, path_to_train_masks, size_256)\n",
    "val_dataset_256 = SegmentationDataset(path_to_val_images, path_to_val_masks, size_256)\n",
    "\n",
    "train_dataset_512 = SegmentationDataset(path_to_train_images, path_to_train_masks, size_512)\n",
    "val_dataset_512 = SegmentationDataset(path_to_val_images, path_to_val_masks, size_512)\n",
    "\n",
    "train_dataset_1024 = SegmentationDataset(path_to_train_images, path_to_train_masks, size_1024)\n",
    "val_dataset_1024 = SegmentationDataset(path_to_val_images, path_to_val_masks, size_1024)\n",
    "\n",
    "\n",
    "# DataLoaders \n",
    "\n",
    "train_dataloader_128 = DataLoader(train_dataset_128, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "val_dataloader_128 = DataLoader(val_dataset_128, num_workers=num_workers, pin_memory=False,batch_size=500, shuffle=True)\n",
    "\n",
    "train_dataloader_256 = DataLoader(train_dataset_256, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "val_dataloader_256 = DataLoader(val_dataset_256, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "\n",
    "train_dataloader_512 = DataLoader(train_dataset_512, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "val_dataloader_512 = DataLoader(val_dataset_512, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "\n",
    "train_dataloader_1024 = DataLoader(train_dataset_1024, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "val_dataloader_1024 = DataLoader(val_dataset_1024, num_workers=num_workers, pin_memory=False, batch_size=500, shuffle=True)\n",
    "\n",
    "model = UNet(in_channels=3, num_classes=2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Start from 128x128 \n",
    "experiment_name_128: str = f\"experiment_{size_128}\"\n",
    "experiment_name_256: str = f\"experiment_{size_256}\"\n",
    "experiment_name_512: str = f\"experiment_{size_512}\"\n",
    "experiment_name_1024: str = f\"experiment_{size_1024}\"\n",
    "\n",
    "# Training\n",
    "train_model(model, optimizer, train_dataloader_128, val_dataloader_128, epochs, experiment_name_128, device)\n",
    "train_model(model, optimizer, train_dataloader_256, val_dataloader_256, epochs, experiment_name_256, device)\n",
    "train_model(model, optimizer, train_dataloader_512, val_dataloader_512, epochs, experiment_name_512, device)\n",
    "train_model(model, optimizer, train_dataloader_1024, val_dataloader_1024, epochs, experiment_name_1024, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
